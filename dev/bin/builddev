#!/bin/bash

# This executes all the commands in cluster-test05Customization.md#DEV

# assuming you followed instructions, this is no longer needed
cd
rm -rf eclipse-installer

hostnamectl set-hostname dev.cluster.test

# I was having performance issues with Pulsar and decided this would be handy
mkdir -p ~/cluster-test/dev/node_exporter/logs
ln -s ~/cluster-test/dev/bin/pnodedown ~/bin/pnodedown
ln -s ~/cluster-test/dev/bin/pnodetail ~/bin/pnodetail
ln -s ~/cluster-test/dev/bin/pnodeup ~/bin/pnodeup

# github
sudo dnf config-manager --add-repo https://cli.github.com/packages/rpm/gh-cli.repo
sudo dnf install gh
# too interactive (and me-based) to run in the script
# gh auth login

# commented out because you don't want to be me (although nothing stops you)
# but I want a convenient reference
#    git config --global user.name "Tiger"
#    git config --global user.email tigersizer@gmail.com

# Docker BuildKit
sudo cp ~/cluster-test/dev/conf/dockerdaemon.json /etc/docker/daemon.json
sudo systemctl restart docker

# pulsar client
sudo pip3 install pulsar-client

# cqlsh (no idea what -U does, just copied)
sudo pip3 install -U cqlsh
ln -s ~/cluster-test/dev/bin/cql ~/bin/cql

# just for documentation purposes, this is required for cassandra (above does it)
# handy for bare Anaconda environments
# sudo pip3 install cassandra-driver

# psql
ln -s ~/cluster-test/dev/bin/psql ~/bin/psql

# Spark
mkdir -p ~/cluster-test/dev/spark/.ivy2
sudo chgrp -R root ~/cluster-test/dev/spark
ln -s ~/cluster-test/dev/bin/spark ~/bin/spark

# PySpark is much more complicated
# conda it
# edit the crap out of the pyspark command (see condapyspark in this directory)
# change the firewall rules, which can be done here
sudo firewall-cmd --zone=public --add-port=4040/tcp --permanent
sudo firewall-cmd --zone=public --add-port=10080/tcp --permanent
sudo firewall-cmd --zone=public --add-port=10081/tcp --permanent

